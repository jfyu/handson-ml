{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components: Singular Value Decomposition\n",
    "build 3D data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X-X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = V.T[:,0]\n",
    "c2 = V.T[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD assumes the original matrix is unitary (i.e. its inverse is also its conjugate-transpose). Since the training set is always real, it means that the inverse is its transpose. That means that the matrix is orthogonal. Hence SVD works on all traning sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA onto the first two principal components\n",
    "W2 = V.T[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Sci-kit learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some excellent explanation is in the jupyter notebook, check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.93636116, -0.29854881, -0.18465208],\n",
       "       [ 0.34027485, -0.90119108, -0.2684542 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_#this gives you the principlal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84248607,  0.14631839])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That tells you that 84% of the variance lies on the first axis, 14.6% lies on the second axis, which means that the third axis only carries <1.2%, which makes sense to leave out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually get up to d dimensions such that 95% of the variance is captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #cumulated sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0979085 ,  0.16935327,  0.230804  ,  0.28502235,  0.33412947,\n",
       "        0.37720032,  0.40992705,  0.43886063,  0.46648008,  0.48986277,\n",
       "        0.5109064 ,  0.53130397,  0.54837612,  0.56532983,  0.58112168,\n",
       "        0.5959951 ,  0.60916632,  0.6219443 ,  0.63381242,  0.64532175,\n",
       "        0.65596806,  0.66608542,  0.67568711,  0.68479722,  0.69358461,\n",
       "        0.7019435 ,  0.70999732,  0.71785028,  0.72523488,  0.73211634,\n",
       "        0.73868069,  0.74512797,  0.75113496,  0.75697298,  0.76264873,\n",
       "        0.76807142,  0.77311152,  0.77795597,  0.78274618,  0.7873923 ,\n",
       "        0.79193513,  0.79636749,  0.80053838,  0.8044898 ,  0.80830943,\n",
       "        0.81206371,  0.81567123,  0.81917095,  0.8225571 ,  0.82575904,\n",
       "        0.82892001,  0.83203043,  0.83498954,  0.83786134,  0.84067219,\n",
       "        0.84336446,  0.84603577,  0.84861641,  0.85114857,  0.85359308,\n",
       "        0.85598998,  0.85836448,  0.86064343,  0.86285353,  0.86498866,\n",
       "        0.86705057,  0.86907432,  0.87100434,  0.87291233,  0.87480417,\n",
       "        0.87667121,  0.87846487,  0.88023273,  0.88195831,  0.88360643,\n",
       "        0.88523364,  0.88685343,  0.88839632,  0.88986959,  0.89128834,\n",
       "        0.89270227,  0.89410046,  0.89549124,  0.89683729,  0.89816532,\n",
       "        0.89948023,  0.90076623,  0.90201778,  0.90323005,  0.90443458,\n",
       "        0.90559555,  0.90673549,  0.90784782,  0.9089486 ,  0.91002479,\n",
       "        0.911098  ,  0.91213137,  0.91316043,  0.9141639 ,  0.91515504,\n",
       "        0.91612767,  0.91706461,  0.9179974 ,  0.91890149,  0.91980462,\n",
       "        0.92069592,  0.92155598,  0.92240608,  0.92324419,  0.92405825,\n",
       "        0.92484317,  0.92561963,  0.92639337,  0.92715779,  0.92791645,\n",
       "        0.92866429,  0.9293912 ,  0.9301139 ,  0.93083007,  0.93153171,\n",
       "        0.93222302,  0.93290579,  0.93358324,  0.93425075,  0.93490896,\n",
       "        0.93555326,  0.93618216,  0.93680822,  0.93742119,  0.93802359,\n",
       "        0.93862376,  0.93921752,  0.93980258,  0.94038318,  0.94096103,\n",
       "        0.94153376,  0.94209553,  0.94264722,  0.94318191,  0.94370798,\n",
       "        0.94422966,  0.94473816,  0.94523792,  0.945735  ,  0.94622888,\n",
       "        0.94671783,  0.9472017 ,  0.94768277,  0.94815296,  0.94861903,\n",
       "        0.94908259,  0.94954303,  0.94999906,  0.9504463 ,  0.9508912 ,\n",
       "        0.95133019,  0.95176828,  0.95219426,  0.9526162 ,  0.95303366,\n",
       "        0.95344405,  0.95385013,  0.95424769,  0.95464284,  0.955034  ,\n",
       "        0.95542161,  0.95580557,  0.95618434,  0.95656063,  0.9569328 ,\n",
       "        0.95730025,  0.957665  ,  0.95802707,  0.95838523,  0.95873811,\n",
       "        0.95908913,  0.95943612,  0.95978024,  0.9601202 ,  0.96045836,\n",
       "        0.96079181,  0.9611206 ,  0.96144755,  0.9617725 ,  0.96209464,\n",
       "        0.96241405,  0.96273072,  0.96304493,  0.96335394,  0.96366269,\n",
       "        0.96396787,  0.96427184,  0.96457306,  0.96487174,  0.96516802,\n",
       "        0.96546355,  0.96575548,  0.96604633,  0.96633266,  0.96661557,\n",
       "        0.96689484,  0.96717188,  0.96744538,  0.96771741,  0.96798598,\n",
       "        0.96825351,  0.96851808,  0.96878132,  0.96904375,  0.96930358,\n",
       "        0.96956104,  0.96981721,  0.97007276,  0.97032584,  0.97057573,\n",
       "        0.9708241 ,  0.9710705 ,  0.97131546,  0.97155762,  0.97179765,\n",
       "        0.97203726,  0.97227582,  0.97251316,  0.97274939,  0.97298113,\n",
       "        0.97321071,  0.97343939,  0.97366679,  0.97389242,  0.97411642,\n",
       "        0.97433751,  0.97455609,  0.9747733 ,  0.97498907,  0.97520434,\n",
       "        0.9754171 ,  0.97562903,  0.97583901,  0.97604704,  0.97625347,\n",
       "        0.97645692,  0.97665905,  0.97685994,  0.97705976,  0.97725868,\n",
       "        0.9774553 ,  0.97765132,  0.97784362,  0.97803573,  0.97822662,\n",
       "        0.97841657,  0.97860586,  0.97879237,  0.97897848,  0.97916409,\n",
       "        0.9793486 ,  0.97953127,  0.97971297,  0.97989423,  0.98007412,\n",
       "        0.98025159,  0.98042802,  0.98060329,  0.98077722,  0.98094967,\n",
       "        0.98112165,  0.98129225,  0.98146168,  0.98163017,  0.98179778,\n",
       "        0.9819642 ,  0.98212996,  0.98229401,  0.98245741,  0.98261989,\n",
       "        0.98278108,  0.9829411 ,  0.98310046,  0.98325901,  0.98341558,\n",
       "        0.98357162,  0.98372676,  0.98388014,  0.98403276,  0.98418362,\n",
       "        0.98433394,  0.98448333,  0.98463092,  0.98477787,  0.98492352,\n",
       "        0.98506832,  0.98521093,  0.98535293,  0.98549456,  0.98563519,\n",
       "        0.98577435,  0.98591125,  0.98604771,  0.98618371,  0.98631904,\n",
       "        0.98645415,  0.98658806,  0.98672121,  0.98685372,  0.98698369,\n",
       "        0.98711263,  0.98724121,  0.98736867,  0.98749572,  0.98762236,\n",
       "        0.98774808,  0.98787349,  0.98799839,  0.98812106,  0.98824291,\n",
       "        0.98836399,  0.98848415,  0.98860342,  0.98872202,  0.98883955,\n",
       "        0.98895698,  0.98907345,  0.98918898,  0.98930377,  0.98941819,\n",
       "        0.98953175,  0.98964328,  0.98975401,  0.98986409,  0.98997365,\n",
       "        0.99008268,  0.99019144,  0.99029965,  0.99040704,  0.99051382,\n",
       "        0.99062013,  0.99072566,  0.99082989,  0.99093288,  0.99103524,\n",
       "        0.99113654,  0.99123655,  0.9913363 ,  0.99143561,  0.99153438,\n",
       "        0.9916323 ,  0.99172991,  0.99182671,  0.99192158,  0.99201559,\n",
       "        0.99210906,  0.99220166,  0.9922937 ,  0.99238501,  0.99247545,\n",
       "        0.99256563,  0.99265511,  0.99274396,  0.99283228,  0.99292029,\n",
       "        0.99300731,  0.9930933 ,  0.99317904,  0.99326364,  0.993348  ,\n",
       "        0.99343167,  0.99351427,  0.99359671,  0.99367835,  0.99375899,\n",
       "        0.99383937,  0.99391842,  0.99399709,  0.99407496,  0.99415245,\n",
       "        0.99422896,  0.99430499,  0.99438038,  0.99445453,  0.99452812,\n",
       "        0.99460096,  0.99467369,  0.99474577,  0.99481728,  0.99488794,\n",
       "        0.99495799,  0.99502744,  0.99509685,  0.99516515,  0.99523246,\n",
       "        0.99529875,  0.99536459,  0.99542929,  0.99549314,  0.99555648,\n",
       "        0.9956193 ,  0.99568083,  0.99574177,  0.99580192,  0.99586147,\n",
       "        0.99592014,  0.99597846,  0.99603664,  0.99609412,  0.9961504 ,\n",
       "        0.99620655,  0.9962619 ,  0.99631649,  0.99637073,  0.99642452,\n",
       "        0.99647697,  0.99652935,  0.99658132,  0.99663264,  0.99668348,\n",
       "        0.99673388,  0.99678402,  0.99683335,  0.99688212,  0.99693045,\n",
       "        0.99697808,  0.99702491,  0.99707113,  0.997117  ,  0.99716269,\n",
       "        0.99720719,  0.99725036,  0.99729291,  0.99733529,  0.9973773 ,\n",
       "        0.99741877,  0.99745974,  0.99750039,  0.99754088,  0.99758113,\n",
       "        0.99761981,  0.99765805,  0.99769577,  0.99773289,  0.99776973,\n",
       "        0.99780622,  0.99784223,  0.99787774,  0.99791302,  0.99794824,\n",
       "        0.9979827 ,  0.99801686,  0.99805083,  0.99808354,  0.99811612,\n",
       "        0.99814816,  0.99817988,  0.99821079,  0.99824154,  0.99827195,\n",
       "        0.99830197,  0.99833157,  0.99836066,  0.9983895 ,  0.99841764,\n",
       "        0.99844568,  0.99847315,  0.99850012,  0.99852636,  0.99855195,\n",
       "        0.99857703,  0.99860184,  0.99862649,  0.99865106,  0.99867533,\n",
       "        0.99869919,  0.99872275,  0.99874609,  0.99876895,  0.99879162,\n",
       "        0.99881388,  0.99883585,  0.99885765,  0.99887933,  0.99890056,\n",
       "        0.99892144,  0.99894219,  0.99896265,  0.99898294,  0.99900286,\n",
       "        0.99902256,  0.99904189,  0.99906093,  0.99907977,  0.99909781,\n",
       "        0.99911572,  0.9991334 ,  0.99915056,  0.99916755,  0.99918388,\n",
       "        0.99919998,  0.99921589,  0.9992317 ,  0.99924742,  0.99926276,\n",
       "        0.99927788,  0.99929259,  0.9993071 ,  0.99932136,  0.99933539,\n",
       "        0.99934918,  0.99936287,  0.99937644,  0.99938997,  0.99940333,\n",
       "        0.99941623,  0.99942883,  0.99944127,  0.99945356,  0.99946569,\n",
       "        0.99947773,  0.99948938,  0.99950096,  0.99951241,  0.99952367,\n",
       "        0.99953474,  0.99954562,  0.99955637,  0.99956693,  0.99957736,\n",
       "        0.99958743,  0.99959728,  0.99960665,  0.99961587,  0.99962493,\n",
       "        0.99963394,  0.99964283,  0.99965167,  0.99966   ,  0.99966803,\n",
       "        0.999676  ,  0.99968392,  0.99969178,  0.99969944,  0.99970703,\n",
       "        0.99971444,  0.99972169,  0.99972885,  0.99973582,  0.99974276,\n",
       "        0.99974955,  0.99975625,  0.99976261,  0.99976886,  0.99977504,\n",
       "        0.99978106,  0.99978687,  0.99979254,  0.99979813,  0.9998037 ,\n",
       "        0.99980917,  0.99981442,  0.99981961,  0.99982457,  0.99982943,\n",
       "        0.99983421,  0.99983886,  0.99984344,  0.99984801,  0.99985254,\n",
       "        0.99985702,  0.99986129,  0.99986548,  0.99986963,  0.99987365,\n",
       "        0.99987754,  0.99988142,  0.99988512,  0.99988879,  0.99989207,\n",
       "        0.99989529,  0.99989835,  0.99990138,  0.9999044 ,  0.99990735,\n",
       "        0.99991026,  0.99991309,  0.99991583,  0.99991856,  0.99992127,\n",
       "        0.99992378,  0.99992628,  0.99992872,  0.99993098,  0.99993322,\n",
       "        0.99993543,  0.99993747,  0.99993947,  0.99994147,  0.99994344,\n",
       "        0.99994536,  0.99994724,  0.99994906,  0.99995083,  0.99995258,\n",
       "        0.99995429,  0.99995599,  0.99995768,  0.99995929,  0.99996082,\n",
       "        0.99996227,  0.99996361,  0.99996493,  0.99996624,  0.99996753,\n",
       "        0.9999687 ,  0.99996986,  0.99997101,  0.99997213,  0.99997325,\n",
       "        0.99997433,  0.9999754 ,  0.99997646,  0.99997746,  0.99997845,\n",
       "        0.99997941,  0.99998028,  0.99998111,  0.99998191,  0.9999827 ,\n",
       "        0.99998348,  0.99998423,  0.99998495,  0.99998564,  0.99998631,\n",
       "        0.99998695,  0.99998759,  0.99998818,  0.99998875,  0.99998928,\n",
       "        0.99998979,  0.99999029,  0.99999073,  0.99999117,  0.9999916 ,\n",
       "        0.99999201,  0.99999241,  0.99999281,  0.99999319,  0.99999356,\n",
       "        0.99999388,  0.99999421,  0.99999454,  0.99999486,  0.99999517,\n",
       "        0.99999548,  0.99999577,  0.99999603,  0.99999627,  0.9999965 ,\n",
       "        0.99999673,  0.99999694,  0.99999714,  0.99999733,  0.99999752,\n",
       "        0.99999769,  0.99999785,  0.99999801,  0.99999816,  0.9999983 ,\n",
       "        0.99999843,  0.99999856,  0.99999869,  0.9999988 ,  0.99999891,\n",
       "        0.99999902,  0.99999911,  0.99999921,  0.9999993 ,  0.99999938,\n",
       "        0.99999945,  0.99999951,  0.99999956,  0.99999961,  0.99999965,\n",
       "        0.99999969,  0.99999973,  0.99999976,  0.99999979,  0.99999982,\n",
       "        0.99999984,  0.99999986,  0.99999988,  0.99999989,  0.99999991,\n",
       "        0.99999992,  0.99999993,  0.99999994,  0.99999995,  0.99999996,\n",
       "        0.99999997,  0.99999998,  0.99999998,  0.99999999,  0.99999999,\n",
       "        0.99999999,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.argmax(cumsum>=0.95)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set n_component to be a float between 0 and 1, indicating the ratio of variance that you wish to preserve\n",
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.85403110e-21,  -8.23993651e-17,  -6.24500451e-17, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  6.84796852e-19,   2.22044605e-16,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [ -2.91018278e-20,  -4.51028104e-17,  -5.55111512e-17, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [ -1.02449824e-18,  -7.71951947e-17,   3.02709247e-16, ...,\n",
       "         -0.00000000e+00,  -0.00000000e+00,  -0.00000000e+00],\n",
       "       [  4.64113634e-18,   5.89805982e-17,   5.55111512e-17, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  7.92439553e-18,  -2.77555756e-17,  -2.15105711e-16, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95044630302001865"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1da504bf98>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1dac6ba400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can inverse transform back to the original\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52500, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_recovered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_pca = IncrementalPCA(n_components=154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch in np.array_split(X_train,n_batches):\n",
    "    inc_pca.partial_fit(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_inc = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X_reduced_inc,X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but mean is equal\n",
    "np.allclose(pca.mean_, inc_pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_rnd = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X_reduced,X_reduced_rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(pca.mean_,rnd_pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2,kernel='rbf',gamma=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_rbf = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    (\"kpca\",KernelPCA(n_components=2)),\n",
    "    (\"log_reg\",LogisticRegression())\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "        \"kpca__gamma\":np.linspace(0.03,0.05,10),\n",
    "        \"kpca__kernel\":[\"rbf\",\"sigmoid\"]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(clf,param_grid,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('kpca', KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',\n",
       "     fit_inverse_transform=False, gamma=None, kernel='linear',\n",
       "     kernel_params=None, max_iter=None, n_components=2, n_jobs=1,\n",
       "     random_state=None, remove_zero_eig=False, tol=0)), ('log_reg', LogisticRegre...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'kpca__gamma': array([ 0.03   ,  0.03222,  0.03444,  0.03667,  0.03889,  0.04111,\n",
       "        0.04333,  0.04556,  0.04778,  0.05   ]), 'kpca__kernel': ['rbf', 'sigmoid']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = t > 6.9\n",
    "grid_search.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruction of pre-image\n",
    "rbf_pca = KernelPCA(n_components=2, kernel='rbf',gamma=0.0433,fit_inverse_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.786308795766139"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X,X_preimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to minimize this error with GridSearch? With a loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Linear Embedding (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jfyu/anaconda2/envs/mlbook/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "rfc_clf.fit(X_train, y_train)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 4.28s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training took {:.2f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94430000000000003"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = rfc_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_clf2 = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "rfc_clf2.fit(X_reduced, y_train)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 10.76s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training took {:.2f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89239999999999997"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "y_pred = rfc_clf2.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See actual notebook. Check with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
    "t0 = time.time()\n",
    "log_clf.fit(X_train, y_train)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 13.76s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training took {:.2f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91830000000000001"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf2 = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
    "t0 = time.time()\n",
    "log_clf2.fit(X_reduced, y_train)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 6.33s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training took {:.2f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91190000000000004"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_clf2.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
